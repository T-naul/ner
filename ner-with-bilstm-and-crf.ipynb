{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install extra-dependencies\n! pip -q install git+https://www.github.com/keras-team/keras-contrib.git sklearn-crfsuite\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-11T16:58:29.519616Z","iopub.execute_input":"2023-05-11T16:58:29.519942Z","iopub.status.idle":"2023-05-11T16:58:45.976130Z","shell.execute_reply.started":"2023-05-11T16:58:29.519889Z","shell.execute_reply":"2023-05-11T16:58:45.974988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nEPOCHS = 5\nMAX_LEN = 75\nEMBEDDING = 100\nMAX_CHAR_LEN = 15\nCHAR_EMBEDDING = 20","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:58:45.978211Z","iopub.execute_input":"2023-05-11T16:58:45.978754Z","iopub.status.idle":"2023-05-11T16:58:45.984281Z","shell.execute_reply.started":"2023-05-11T16:58:45.978694Z","shell.execute_reply":"2023-05-11T16:58:45.983188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reading the data\ndata = pd.read_csv(\"/kaggle/input/entity-annotated-corpus/ner_dataset.csv\", encoding=\"latin1\")\ndata = data.fillna(method=\"ffill\")\nprint(\"Number of sentences: \", len(data.groupby(['Sentence #'])))\n\nwords = list(set(data[\"Word\"].values))\nn_words = len(words)\nprint(\"Number of words in the dataset: \", n_words)\n\ntags = list(set(data[\"Tag\"].values))\nprint(\"Tags:\", tags)\n\nn_tags = len(tags)\nprint(\"Number of Labels: \", n_tags)\n\nprint(\"What the dataset looks like:\")\n# Show the first 10 rows\ndata.head(n=10)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:58:45.985868Z","iopub.execute_input":"2023-05-11T16:58:45.986373Z","iopub.status.idle":"2023-05-11T16:58:50.482107Z","shell.execute_reply.started":"2023-05-11T16:58:45.986180Z","shell.execute_reply":"2023-05-11T16:58:50.480230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data preprocessing\nclass SentenceGetter(object):\n    \"\"\"Class to Get the sentence in this format:\n    [(Token_1, Part_of_Speech_1, Tag_1), ..., (Token_n, Part_of_Speech_1, Tag_1)]\"\"\"\n    def __init__(self, data):\n        \"\"\"Args:\n            data is the pandas.DataFrame which contains the above dataset\"\"\"\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n                                                           s[\"POS\"].values.tolist(),\n                                                           s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        \"\"\"Return one sentence\"\"\"\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None\n        \ngetter = SentenceGetter(data)\nsent = getter.get_next()\nprint('This is what a sentence looks like:')\nprint(sent)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:58:50.483515Z","iopub.execute_input":"2023-05-11T16:58:50.483847Z","iopub.status.idle":"2023-05-11T16:58:57.514397Z","shell.execute_reply.started":"2023-05-11T16:58:50.483793Z","shell.execute_reply":"2023-05-11T16:58:57.513392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Vocabulary Key:word -> Value:token_index\n# The first 2 entries are reserved for PAD and UNK\nword2idx = {w: i + 2 for i, w in enumerate(words)}\nword2idx[\"UNK\"] = 1 # Unknown words\nword2idx[\"PAD\"] = 0 # Padding\n\n# Vocabulary Key:token_index -> Value:word\nidx2word = {i: w for w, i in word2idx.items()}\n\n# Vocabulary Key:Label/Tag -> Value:tag_index\n# The first entry is reserved for PAD\ntag2idx = {t: i+1 for i, t in enumerate(tags)}\ntag2idx[\"PAD\"] = 0\n\n# Vocabulary Key:tag_index -> Value:Label/Tag\nidx2tag = {i: w for w, i in tag2idx.items()}\nprint(\"The word Obama is identified by the index: {}\".format(word2idx[\"Obama\"]))\nprint(\"The labels B-geo(which defines Geopraphical Enitities) is identified by the index: {}\".format(tag2idx[\"B-geo\"]))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:58:57.518135Z","iopub.execute_input":"2023-05-11T16:58:57.518477Z","iopub.status.idle":"2023-05-11T16:58:57.544796Z","shell.execute_reply.started":"2023-05-11T16:58:57.518401Z","shell.execute_reply":"2023-05-11T16:58:57.543840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Character-level Vocabulary Key:char -> Value:char_index\n# The first 2 entries are reserved for PAD and UNK\nchars = set([w_i for w in words for w_i in w])\nn_chars = len(chars)\nchar2idx = {c: i + 2 for i, c in enumerate(chars)}\nchar2idx[\"UNK\"] = 1 # Unknown characters\nchar2idx[\"PAD\"] = 0 # Padding","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:58:57.547875Z","iopub.execute_input":"2023-05-11T16:58:57.548356Z","iopub.status.idle":"2023-05-11T16:58:57.574938Z","shell.execute_reply.started":"2023-05-11T16:58:57.548150Z","shell.execute_reply":"2023-05-11T16:58:57.574131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Character-level Vocabulary Key:char_index -> Value:char\nchar2idx = {\"PAD\": 0, \"UNK\": 1}\nfor word in words:\n    for char in word:\n        if char not in char2idx:\n            char2idx[char] = len(char2idx)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:58:57.576528Z","iopub.execute_input":"2023-05-11T16:58:57.577015Z","iopub.status.idle":"2023-05-11T16:58:57.620132Z","shell.execute_reply.started":"2023-05-11T16:58:57.576819Z","shell.execute_reply":"2023-05-11T16:58:57.619287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = getter.sentences","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:58:57.621725Z","iopub.execute_input":"2023-05-11T16:58:57.622162Z","iopub.status.idle":"2023-05-11T16:58:57.626670Z","shell.execute_reply.started":"2023-05-11T16:58:57.622000Z","shell.execute_reply":"2023-05-11T16:58:57.625857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert each word to a list of character indices\nX_char = []\nfor sentence in sentences:\n    sent_seq = []\n    for i in range(MAX_LEN):\n        word_seq = []\n        for j in range(MAX_CHAR_LEN):\n            try:\n                char = sentence[i][0][j]\n                word_seq.append(char2idx[char])\n            except:\n                word_seq.append(char2idx[\"PAD\"])\n        sent_seq.append(word_seq)\n    X_char.append(np.array(sent_seq))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:58:57.628484Z","iopub.execute_input":"2023-05-11T16:58:57.629103Z","iopub.status.idle":"2023-05-11T16:59:42.992660Z","shell.execute_reply.started":"2023-05-11T16:58:57.628832Z","shell.execute_reply":"2023-05-11T16:59:42.991835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import concatenate\n# Convert each sentence from list of Token to list of word_index\nX_word = [[word2idx[w[0]] for w in s] for s in sentences]\n\n# Padding each sentence to have the same lenght\nX_word = pad_sequences(maxlen=MAX_LEN, sequences=X_word, padding=\"post\", value=word2idx[\"PAD\"])\n# Padding each sentence to have the same length\nX_char = pad_sequences(maxlen=MAX_LEN, sequences=X_char, padding=\"post\", value=char2idx[\"PAD\"])\n\n# combine word and char sequences into one input array\n#X = np.c_[X_word.reshape(len(X_word),-1), X_char.reshape(len(X_char),-1)]\n#X = concatenate([X_word,X_char])","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:42.994109Z","iopub.execute_input":"2023-05-11T16:59:42.994403Z","iopub.status.idle":"2023-05-11T16:59:44.297757Z","shell.execute_reply.started":"2023-05-11T16:59:42.994356Z","shell.execute_reply":"2023-05-11T16:59:44.296854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert Tag/Label to tag_index\ny = [[tag2idx[w[2]] for w in s] for s in sentences]\n# Padding each sentence to have the same lenght\ny = pad_sequences(maxlen=MAX_LEN, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\nfrom keras.utils import to_categorical\n# One-Hot encode\ny = [to_categorical(i, num_classes=n_tags+1) for i in y]  # n_tags+1(PAD)\n\nfrom sklearn.model_selection import train_test_split\nX_word_tr, X_word_te, X_char_tr, X_char_te, y_tr, y_te = train_test_split(X_word,X_char, y, test_size=0.2)\n#X_tr.shape, X_te.shape, np.array(y_tr).shape, np.array(y_te).shape\n#print('Raw Sample: ', ' '.join([w[0] for w in sentences[0]]))\n#print('Raw Label: ', ' '.join([w[2] for w in sentences[0]]))\n#print('After processing, sample:', X[0])\n#print('After processing, labels:', y[0])","metadata":{"execution":{"iopub.status.busy":"2023-05-11T17:01:29.804111Z","iopub.execute_input":"2023-05-11T17:01:29.804461Z","iopub.status.idle":"2023-05-11T17:01:31.537438Z","shell.execute_reply.started":"2023-05-11T17:01:29.804405Z","shell.execute_reply":"2023-05-11T17:01:31.536551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the pre-trained GloVe embeddings\nembeddings_index = {}\nwith open('/kaggle/input/glove6b100dtxt/glove.6B.100d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs","metadata":{"execution":{"iopub.status.busy":"2023-05-11T17:01:35.438359Z","iopub.execute_input":"2023-05-11T17:01:35.438705Z","iopub.status.idle":"2023-05-11T17:01:52.939298Z","shell.execute_reply.started":"2023-05-11T17:01:35.438646Z","shell.execute_reply":"2023-05-11T17:01:52.938393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the model\ninput_dim = n_words + 2\nembedding_matrix = np.zeros((input_dim, 100))\nfor i,word in idx2word.items():\n    embedding_vector = embeddings_index.get(word.lower)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.863578Z","iopub.status.idle":"2023-05-11T16:59:46.864294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Conv1D, MaxPooling1D, Flatten\n#from tensorflow.keras.layers import \nfrom keras_contrib.layers import CRF\nimport keras as k","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.865535Z","iopub.status.idle":"2023-05-11T16:59:46.866249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model definition\n# define word-level input\ninput = Input(shape=(MAX_LEN,))\n\nmodel = Embedding(input_dim=n_words+2, output_dim=EMBEDDING, # n_words + 2 (PAD & UNK)\n                  weights=[embedding_matrix],input_length=MAX_LEN)(input)  # default: 100-dim embedding\n\n# Creating character-level embeddings\ninput_char = Input(shape=(MAX_LEN, MAX_CHAR_LEN,))\nchar_emb = TimeDistributed(Embedding(len(char2idx), CHAR_EMBEDDING))(input_char)\nchar_emb = TimeDistributed(Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))(char_emb)\nchar_emb = TimeDistributed(MaxPooling1D(pool_size=2))(char_emb)\nchar_emb = TimeDistributed(Flatten())(char_emb)\nchar_emb = Dropout(0.5)(char_emb)\n\n# Concatenate word and character embeddings\nmodel = concatenate([model, char_emb])\n\nmodel = Bidirectional(LSTM(units=50, return_sequences=True,\n                           dropout=0.5, \n                           recurrent_dropout=0.5,\n                          kernel_initializer=k.initializers.he_normal()))(model)  # variational biLSTM\nmodel = LSTM(units=50 * 2, \n             return_sequences=True, \n             dropout=0.5, \n             recurrent_dropout=0.5, \n             kernel_initializer=k.initializers.he_normal())(model)\nmodel = TimeDistributed(Dense(100, activation=\"relu\"))(model)  # a dense layer as suggested by neuralNer\ncrf = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\nout = crf(model)  # output\nmodel = Model(inputs=[input, input_char], outputs=out)\n#Optimiser \nadam = k.optimizers.Adam(lr=0.0008, beta_1=0.9, beta_2=0.999)\n# Compile model\nmodel.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.867519Z","iopub.status.idle":"2023-05-11T16:59:46.868234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n# Saving the best model only\nfilepath=\"ner-bi-lstm-td-model-{val_accuracy:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\n\n# Fit the best model\nhistory = model.fit([X_word_tr,X_char_tr], np.array(y_tr), batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1, verbose=1, callbacks=callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.869487Z","iopub.status.idle":"2023-05-11T16:59:46.870300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the graph \nplt.style.use('ggplot')\n\ndef plot_history(history):\n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(accuracy) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, accuracy, 'b', label='Training acc')\n    plt.plot(x, val_accuracy, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\nplot_history(history)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.871930Z","iopub.status.idle":"2023-05-11T16:59:46.872744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred2label(pred):\n    out = []\n    for pred_i in pred:\n        out_i = []\n        for p in pred_i:\n            p_i = np.argmax(p)\n            out_i.append(idx2tag[p_i])\n        out.append(out_i)\n    return out\ntest_pred = model.predict([X_word_te,X_char_te], verbose=1)   \npred_labels = pred2label(test_pred)\ntest_labels = pred2label(y_te)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.874266Z","iopub.status.idle":"2023-05-11T16:59:46.875037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.876270Z","iopub.status.idle":"2023-05-11T16:59:46.876986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\nprint(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.878232Z","iopub.status.idle":"2023-05-11T16:59:46.878950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install sklearn_crfsuite","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.880282Z","iopub.status.idle":"2023-05-11T16:59:46.880995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from  sklearn_crfsuite.metrics import flat_classification_report  \nreport = flat_classification_report(y_pred=pred_labels, y_true=test_labels)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.882427Z","iopub.status.idle":"2023-05-11T16:59:46.883122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = \"President Obama became the first sitting American president to visit Hiroshima\"","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.884347Z","iopub.status.idle":"2023-05-11T16:59:46.885043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nre_tok = re.compile(f\"([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])\")\n\nsentence = re_tok.sub(r\"  \", str(sentence)).split()\npadded_sentence = sentence + [word2idx[\"PAD\"]]*(MAX_LEN - len(sentence))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.886288Z","iopub.status.idle":"2023-05-11T16:59:46.886984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_char = []\nfor sentence in padded_sentence:\n    temp=[]\n    for i in sentence:\n        temp.append(char2idx[i])\n    temp.append(char2idx[\"PAD\"]*(MAX_CHAR_LEN - len(temp)))\n    X_char.append(temp)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.888226Z","iopub.status.idle":"2023-05-11T16:59:46.888920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"padded_sentence = [word2idx.get(w, 0) for w in padded_sentence]\n\npred = model.predict(np.array([padded_sentence,X_char]))\npred = np.argmax(pred, axis=-1)\n\nretval = \"\"\nfor w, p in zip(sentence, pred[0]):\n    retval = retval + \"{:15}: {:5}\".format(w, idx2tag[p]) + \"\\n\"\nprint(retval)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.890167Z","iopub.status.idle":"2023-05-11T16:59:46.890952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\ncrf2 = CRF(n_tags+1)  # CRF layer, n_tags+1(PAD)\nmodel2 = load_model('/kaggle/working/ner-bi-lstm-td-model-0.99.hdf5', custom_objects={'CRF':CRF,'crf_loss':crf2.loss_function, 'crf_viterbi_accuracy':crf2.accuracy})","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.892335Z","iopub.status.idle":"2023-05-11T16:59:46.893032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model2.predict(np.array([padded_sentence]))\npred = np.argmax(pred, axis=-1)\n\nretval = \"\"\nfor w, p in zip(sentence, pred[0]):\n    retval = retval + \"{:15}: {:5}\".format(w, idx2tag[p]) + \"\\n\"\nprint(retval)","metadata":{"execution":{"iopub.status.busy":"2023-05-11T16:59:46.894268Z","iopub.status.idle":"2023-05-11T16:59:46.894953Z"},"trusted":true},"execution_count":null,"outputs":[]}]}